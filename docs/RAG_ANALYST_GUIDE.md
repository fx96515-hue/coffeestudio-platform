# RAG AI Analyst Guide

## Ãœbersicht

Der RAG (Retrieval-Augmented Generation) KI-Analyst ist eine conversational AI fÃ¼r die CoffeeStudio-Plattform. Nutzer kÃ¶nnen in natÃ¼rlicher Sprache (Deutsch/Englisch) Fragen Ã¼ber Kooperativen, RÃ¶stereien, Marktdaten und Sourcing stellen.

## Architektur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Frontend (Next.js)                      â”‚
â”‚                  /analyst Chat Interface                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ HTTP/JSON
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Backend API (FastAPI)                          â”‚
â”‚           POST /analyst/ask                                 â”‚
â”‚           GET  /analyst/status                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            RAGAnalystService                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  1. Question Embedding                               â”‚  â”‚
â”‚  â”‚     (via EmbeddingService)                           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                      â–¼                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  2. Context Retrieval                                â”‚  â”‚
â”‚  â”‚     (pgvector Similarity Search)                     â”‚  â”‚
â”‚  â”‚     - Top N Cooperatives                             â”‚  â”‚
â”‚  â”‚     - Top N Roasters                                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                      â–¼                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  3. System Prompt Building                           â”‚  â”‚
â”‚  â”‚     (Context + Instructions)                         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                      â–¼                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  4. LLM Generation                                   â”‚  â”‚
â”‚  â”‚     (OpenAI GPT-4o-mini)                             â”‚  â”‚
â”‚  â”‚     System + History + Question â†’ Answer             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PostgreSQL + pgvector                          â”‚
â”‚     cooperatives.embedding (HNSW Index)                     â”‚
â”‚     roasters.embedding (HNSW Index)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Komponenten

### Backend

#### 1. Service: `backend/app/services/rag_analyst.py`

**RAGAnalystService** - Hauptlogik fÃ¼r RAG-basierte Fragenbeantwortung

**Methoden:**
- `is_available()` - PrÃ¼ft ob OpenAI API Key konfiguriert ist
- `ask(question, conversation_history, db)` - Beantwortet Fragen mit RAG
- `_retrieve_context(question, db)` - Holt relevante Entities via pgvector
- `_build_system_prompt(context)` - Baut System-Prompt mit Kontext

**Konfiguration:**
- Nutzt `settings.RAG_LLM_MODEL` (default: "gpt-4o-mini")
- Nutzt `settings.RAG_TEMPERATURE` (default: 0.3)
- Nutzt `settings.RAG_MAX_CONTEXT_ENTITIES` (default: 10)
- Nutzt `settings.RAG_MAX_CONVERSATION_HISTORY` (default: 20)

#### 2. API Routes: `backend/app/api/routes/rag_analyst.py`

**Endpoints:**

**POST /analyst/ask**
- Fragt den KI-Analysten
- Rate Limit: 20 Requests/Minute
- Authentifizierung: `require_auth` (alle authentifizierten Nutzer)
- Request Body: `RAGQuestion`
- Response: `RAGResponse`

**GET /analyst/status**
- Gibt Status des RAG Service zurÃ¼ck
- Authentifizierung: `require_auth`
- Response: `RAGStatusResponse`

#### 3. Schemas: `backend/app/schemas/rag_analyst.py`

**RAGQuestion**
```python
{
  "question": str,  # max 1000 chars
  "conversation_history": [
    {"role": "user" | "assistant", "content": str}
  ]  # max 20 messages
}
```

**RAGResponse**
```python
{
  "answer": str,
  "sources": [
    {
      "entity_type": "cooperative" | "roaster",
      "entity_id": int,
      "name": str,
      "similarity_score": float  # 0.0-1.0
    }
  ],
  "model": str,
  "tokens_used": int | None
}
```

**RAGStatusResponse**
```python
{
  "available": bool,
  "model": str,
  "embedding_model": str
}
```

### Frontend

#### Analyst Page: `frontend/app/analyst/page.tsx`

**Features:**
- Chat-Interface mit User/Assistant Messages
- Beispielfragen zum Anklicken
- Loading-Spinner wÃ¤hrend der Antwort
- Quellenangaben als klickbare Links
- Conversation History (automatisch mitgesendet)
- Error Handling und Service Status Check
- Responsive Design mit Kaffee-Theme

**Styled Components:**
- Warme BrauntÃ¶ne passend zum CoffeeStudio-Design
- CSS-Variablen aus `globals.css`
- Mobile-friendly Layout

## API Beispiele

### Frage stellen

```bash
curl -X POST http://localhost:8000/analyst/ask \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Welche Kooperativen in Cajamarca haben Fair Trade Zertifizierung?",
    "conversation_history": []
  }'
```

**Response:**
```json
{
  "answer": "In Cajamarca gibt es mehrere Kooperativen mit Fair Trade Zertifizierung:\n\n1. **Cooperativa Agraria Cafetalera La Prosperidad** (ID: 123)\n   - Region: Cajamarca\n   - Zertifizierungen: Organic, Fair Trade\n   - HÃ¶he: 1500m\n\n2. **Cooperativa Agraria Cafetalera San Ignacio** (ID: 456)...",
  "sources": [
    {
      "entity_type": "cooperative",
      "entity_id": 123,
      "name": "Cooperativa Agraria Cafetalera La Prosperidad",
      "similarity_score": 0.89
    },
    {
      "entity_type": "cooperative",
      "entity_id": 456,
      "name": "Cooperativa Agraria Cafetalera San Ignacio",
      "similarity_score": 0.85
    }
  ],
  "model": "gpt-4o-mini",
  "tokens_used": 450
}
```

### Status prÃ¼fen

```bash
curl -X GET http://localhost:8000/analyst/status \
  -H "Authorization: Bearer YOUR_TOKEN"
```

**Response:**
```json
{
  "available": true,
  "model": "gpt-4o-mini",
  "embedding_model": "text-embedding-3-small"
}
```

### Mit Conversation History

```bash
curl -X POST http://localhost:8000/analyst/ask \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Welche davon sind in Ã¼ber 1600m HÃ¶he?",
    "conversation_history": [
      {
        "role": "user",
        "content": "Welche Kooperativen in Cajamarca haben Fair Trade?"
      },
      {
        "role": "assistant",
        "content": "In Cajamarca gibt es mehrere Kooperativen..."
      }
    ]
  }'
```

## Konfiguration

### Umgebungsvariablen

In `.env` oder `.env.local`:

```bash
# Required: OpenAI API Key
OPENAI_API_KEY=sk-...

# Optional: RAG Configuration
RAG_LLM_MODEL=gpt-4o-mini          # LLM model to use
RAG_MAX_CONTEXT_ENTITIES=10        # Max entities for context
RAG_MAX_CONVERSATION_HISTORY=20    # Max conversation messages
RAG_TEMPERATURE=0.3                # LLM temperature (0.0-2.0)

# Required for embeddings
EMBEDDING_MODEL=text-embedding-3-small
```

### Settings in `backend/app/core/config.py`

```python
class Settings(BaseSettings):
    # OpenAI for embeddings and RAG
    OPENAI_API_KEY: str | None = None
    EMBEDDING_MODEL: str = "text-embedding-3-small"
    
    # RAG AI Analyst
    RAG_LLM_MODEL: str = "gpt-4o-mini"
    RAG_MAX_CONTEXT_ENTITIES: int = 10
    RAG_MAX_CONVERSATION_HISTORY: int = 20
    RAG_TEMPERATURE: float = 0.3
```

## Frontend-Nutzung

1. **Zugriff:** Navigiere zu `/analyst` in der Sidebar unter "ğŸ¤– KI-Analyst"

2. **Beispielfragen:** Klicke auf eine der vorgeschlagenen Fragen:
   - "Welche Kooperativen in Cajamarca haben Fair Trade Zertifizierung?"
   - "Vergleiche RÃ¶stereien in MÃ¼nchen nach Bewertung"
   - "Was sind die besten Regionen fÃ¼r Specialty Coffee in Peru?"

3. **Eigene Fragen:** Tippe deine Frage ins Eingabefeld und klicke "Senden"

4. **Quellenangaben:** Klicke auf Quellenlinks um zu den Entity-Detailseiten zu gelangen

5. **Conversation History:** Die letzten 20 Nachrichten werden automatisch als Kontext mitgesendet

## Troubleshooting

### Service nicht verfÃ¼gbar (503)

**Problem:** API gibt 503 zurÃ¼ck mit "RAG AI Analyst ist nicht verfÃ¼gbar"

**LÃ¶sung:**
- PrÃ¼fe ob `OPENAI_API_KEY` in `.env` gesetzt ist
- PrÃ¼fe API Key GÃ¼ltigkeit auf https://platform.openai.com/api-keys
- Restart Backend: `docker-compose restart backend`

### Keine Embeddings fÃ¼r Entities

**Problem:** Queries funktionieren nicht, da keine Embeddings vorhanden

**LÃ¶sung:**
```bash
# Embeddings fÃ¼r alle Cooperatives generieren
curl -X POST http://localhost:8000/enrich/cooperatives/embeddings \
  -H "Authorization: Bearer YOUR_ADMIN_TOKEN"

# Embeddings fÃ¼r alle Roasters generieren
curl -X POST http://localhost:8000/enrich/roasters/embeddings \
  -H "Authorization: Bearer YOUR_ADMIN_TOKEN"
```

### Rate Limit erreicht

**Problem:** "Too Many Requests" Error

**LÃ¶sung:**
- Warte 1 Minute (Rate Limit: 20 Requests/Minute)
- Reduziere Anzahl der Anfragen
- Bei Bedarf: ErhÃ¶he Limit in `backend/app/api/routes/rag_analyst.py`

### Schlechte AntwortqualitÃ¤t

**Problem:** Antworten sind ungenau oder irrelevant

**MÃ¶gliche Ursachen:**
1. **Zu wenig Kontext:** ErhÃ¶he `RAG_MAX_CONTEXT_ENTITIES`
2. **Schlechte Embeddings:** Regeneriere Embeddings fÃ¼r Entities
3. **Falsches Modell:** Wechsle zu `gpt-4` fÃ¼r bessere QualitÃ¤t (teurer)
4. **Zu kreativ:** Reduziere `RAG_TEMPERATURE` auf 0.1-0.2

**Anpassungen in `.env`:**
```bash
RAG_MAX_CONTEXT_ENTITIES=15
RAG_TEMPERATURE=0.2
RAG_LLM_MODEL=gpt-4  # Bessere QualitÃ¤t, aber teurer
```

### OpenAI API Fehler

**Problem:** API gibt Fehler zurÃ¼ck (429, 500, etc.)

**LÃ¶sung:**
- 429 Rate Limit: Warte und versuche erneut
- 500 Server Error: Retry nach ein paar Sekunden
- 401 Unauthorized: API Key Ã¼berprÃ¼fen
- PrÃ¼fe OpenAI Status: https://status.openai.com/

### Frontend zeigt "Fehler beim Verbinden"

**Problem:** Frontend kann Backend nicht erreichen

**LÃ¶sung:**
- PrÃ¼fe `NEXT_PUBLIC_API_URL` in Frontend `.env.local`
- PrÃ¼fe Backend lÃ¤uft: `curl http://localhost:8000/health`
- PrÃ¼fe CORS Settings in Backend Config
- PrÃ¼fe Browser Console fÃ¼r Details

## Kosten und Performance

### Token-Kosten

- **Embeddings:** ~$0.0001 pro Request (text-embedding-3-small)
- **LLM Calls:** Variiert je nach Modell
  - gpt-4o-mini: ~$0.0003 per 1K input tokens, ~$0.0012 per 1K output tokens
  - gpt-4: ~$0.03 per 1K input tokens, ~$0.06 per 1K output tokens

**Typische Kosten pro Frage:**
- Embedding: ~$0.0001
- LLM (mit 5 Context Entities): ~$0.001-0.003 (gpt-4o-mini)
- **Total:** ~$0.001-0.003 pro Frage

### Performance Optimierung

1. **Cache hÃ¤ufige Fragen** (TODO: Redis Cache)
2. **Reduziere Context:** Weniger Entities = schneller + billiger
3. **Batch Embedding Generation:** Nutze Celery Tasks
4. **HNSW Index Tuning:** Optimiere pgvector Index Parameter

## Enterprise Roadmap

Siehe Issue #85 fÃ¼r geplante Features:
- Multi-Modal Support (PDFs, Bilder)
- Custom Knowledge Base Integration
- Advanced Analytics Dashboard
- Multi-Language Support (Spanisch, etc.)
- Voice Input/Output
- Export von Conversations

## Support

- **Technische Fragen:** Siehe [BACKEND_SETUP.md](../BACKEND_SETUP.md)
- **API Docs:** http://localhost:8000/docs (Swagger UI)
- **Logs:** `docker-compose logs backend` fÃ¼r Debugging
- **Issues:** GitHub Issues fÃ¼r Bug Reports

---

**Version:** 1.0.0  
**Letzte Aktualisierung:** 2026-02-13  
**Maintainer:** CoffeeStudio Platform Team

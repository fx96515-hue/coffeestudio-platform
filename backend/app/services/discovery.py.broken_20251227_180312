from __future__ import annotations
from typing import Any, cast

import json
import re
from datetime import datetime

    

from sqlalchemy.orm import Session
from sqlalchemy import select, func

from app.models.cooperative import Cooperative
from app.models.roaster import Roaster
from app.models.source import Source
from app.models.evidence import EntityEvidence
from app.providers.perplexity import PerplexityClient, safe_json_loads, PerplexityError


def _entities_response_format() -> dict[str, Any]:
    """Perplexity structured output (JSON Schema) to enforce strict JSON."""
    schema = {
        "type": "object",
        "properties": {
            "entities": {
                "type": "array",
                "maxItems": 20,
                "items": {
                    "type": "object",
                    "properties": {
                        "name": {"type": "string"},
                        "country": {"type": ["string", "null"]},
                        "region": {"type": ["string", "null"]},
                        "website": {"type": ["string", "null"]},
                        "contact_email": {"type": ["string", "null"]},
                        "notes": {"type": ["string", "null"]},
                        "evidence_urls": {"type": "array", "items": {"type": "string"}},
                    },
                    "required": ["name", "evidence_urls"],
                    "additionalProperties": False,
                },
            }
        },
        "required": ["entities"],
        "additionalProperties": False,
    }
    return {"type": "json_schema", "json_schema": {"schema": schema}}


def _repair_json_with_llm(client: PerplexityClient, raw: str) -> dict[str, Any]:
    """Ask the model to repair malformed JSON into strict JSON.

    This is a pragmatic guardrail: some model outputs are almost-JSON
    (missing commas, trailing commas, stray text). We re-prompt for a
    strict JSON object only.
    """
    system = (
        "Du bist ein JSON-Repair-Bot. Du bekommst fehlerhaften JSON-Text und gibst NUR valides JSON zur\u00fcck. "
        "Keine Erkl\u00e4rungen, kein Markdown. Verwende doppelte Anf\u00fchrungszeichen. "
        "Wenn Inhalte fehlen oder abgeschnitten sind, entferne die kaputten Teile statt zu raten. "
        "Output MUSS strikt parsebar sein (RFC8259) und genau ein JSON-Objekt enthalten."
    )
    content = client.chat_completions(
        messages=[
            {"role": "system", "content": system},
            {"role": "user", "content": raw[:12000]},
        ],
        temperature=0.0,
        max_tokens=1800,
        response_format=_entities_response_format(),
    )
    data = safe_json_loads(content)
    if not isinstance(data, dict):
        raise ValueError("JSON repair returned non-object")
    return data


def _norm_name(name: str) -> str:
    return re.sub(r"[^a-z0-9]+", "", name.strip().lower())


def _get_or_create_source(
    db: Session,
    name: str,
    url: str | None,
    kind: str = "api",
    reliability: float | None = 0.6,
) -> Source:
    stmt = select(Source).where(func.lower(Source.name) == name.lower())
    src = db.scalar(stmt)
    if src:
        return src
    src = Source(name=name, url=url, kind=kind, reliability=reliability)
    db.add(src)
    db.commit()
    db.refresh(src)
    return src


COOP_QUERIES = [
    "Peru coffee cooperative exporter list",
    "cooperativa cafetalera peru exportadora",
    "cooperativa cafe peru fairtrade organic",
    "central de cooperativas caf\u00e9 Per\u00fa exportaci\u00f3n",
    "Peru coffee cooperative Cajamarca",
    "Peru coffee cooperative Junin Satipo",
    "Peru coffee cooperative Puno Sandia",
]

ROASTER_QUERIES = [
    "specialty coffee roaster Germany",
    "Kaffeer\u00f6sterei Deutschland specialty direct trade",
    "Third Wave coffee roastery Deutschland",
    "R\u00f6sterei Berlin specialty coffee",
    "R\u00f6sterei M\u00fcnchen specialty coffee",
    "R\u00f6sterei Hamburg specialty coffee",
]


def _extract_entities_with_llm(
    client: PerplexityClient,
    *,
    entity_type: str,
    search_results: list[dict[str, Any]],
    language: str = "de",
) -> list[dict[str, Any]]:
    # NOTE: Do NOT use `.format()` on strings that contain JSON-like braces.
    # It will attempt to substitute fields like {"entities"...} and crash with KeyError("\"entities\"").
    system = (
        "Du extrahierst strukturierte Entit\u00e4ten aus Suchergebnissen. "
        "Gib NUR valides JSON zur\u00fcck (kein Markdown, keine Erkl\u00e4rungen). "
        "Gib GENAU ein JSON-Objekt zur\u00fcck mit dem Feld 'entities'. "
        'Schema: {"entities":[{"name":string,"country":string|null,"region":string|null,"website":string|null,"contact_email":string|null,"notes":string|null,"evidence_urls":[string]}]} '
        f"Regeln: (1) nichts erfinden; unbekannt => null/[]; (2) nur echte {entity_type} im Ziel-Land; "
        "(3) Duplikate entfernen; (4) evidence_urls nur aus den gelieferten URLs; (5) max 20 entities; "
        "(6) KEINE trailing commas."
    )

    user = {
        "entity_type": entity_type,
        "target": "Peru" if entity_type == "cooperative" else "Germany",
        "results": search_results,
    }
    content = client.chat_completions(
        messages=[
            {"role": "system", "content": system},
            {"role": "user", "content": json.dumps(user, ensure_ascii=False)},
        ],
        temperature=0.0,
        max_tokens=2200,
        response_format=_entities_response_format(),
    )
    try:
        data = safe_json_loads(content)
    except Exception:
        # Pragmatic: repair malformed JSON using the model.
        data = _repair_json_with_llm(client, content)
    ents = data.get("entities") or []
    if not isinstance(ents, list):
        return []
    cleaned: list[dict[str, Any]] = []
    for ent in ents:
        if not isinstance(ent, dict):
            continue
        name = (ent.get("name") or "").strip()
        if not name:
            continue
        evidence_urls = ent.get("evidence_urls") or []
        if not isinstance(evidence_urls, list):
            evidence_urls = []
        cleaned.append(
            {
                "name": name,
                "country": ent.get("country"),
                "region": ent.get("region"),
                "website": ent.get("website"),
                "contact_email": ent.get("contact_email"),
                "notes": ent.get("notes"),
                "evidence_urls": [u for u in evidence_urls if isinstance(u, str)],
            }
        )
    # Dedup by normalized name
    out: list[dict[str, Any]] = []
    seen: set[str] = set()
    for ent in cleaned:
        k = _norm_name(ent["name"])
        if k in seen:
            continue
        seen.add(k)
        out.append(ent)
    return out


def seed_discovery(
    db: Session,
    *,
    entity_type: str,
    max_entities: int = 200,
    dry_run: bool = False,
    country_filter: str | None = None,
) -> dict[str, Any]:
    """Discover cooperatives or roasters using Perplexity Search + extraction.

    This is intentionally conservative: we store evidence URLs and mark items as "In Recherche".
    """
    if entity_type not in {"cooperative", "roaster"}:
        raise ValueError("entity_type must be cooperative|roaster")

    src = _get_or_create_source(
        db,
        name="Perplexity Discovery",
        url="https://docs.perplexity.ai/",
        kind="api",
        reliability=0.6,
    )

    queries = COOP_QUERIES if entity_type == "cooperative" else ROASTER_QUERIES
    default_country = "PE" if entity_type == "cooperative" else "DE"
    country = country_filter or default_country

    created = 0
    updated = 0
    skipped = 0
    errors: list[str] = []

    client = PerplexityClient()
    try:
        # Collect search results across queries
        aggregated: list[dict[str, Any]] = []
        seen_urls: set[str] = set()
        for q in queries:
            if len(aggregated) >= 120:
                break
            try:
                results = client.search(
                    q, max_results=20, country=country, max_tokens_per_page=512
                )
                for r in results:
                    if r.url in seen_urls:
                        continue
                    seen_urls.add(r.url)
                    aggregated.append(
                        {"title": r.title, "url": r.url, "snippet": r.snippet}
                    )
            except Exception as e:
                errors.append(f"search failed for '{q}': {e}")

        # Extract entities from aggregated results (in chunks)
        entities: list[dict[str, Any]] = []
        chunk_size = 20
        for i in range(0, len(aggregated), chunk_size):
            chunk = aggregated[i : i + chunk_size]
            if not chunk:
                continue
            try:
                ents = _extract_entities_with_llm(
                    client, entity_type=entity_type, search_results=chunk
                )
                entities.extend(ents)
            except Exception as e:
                errors.append(f"extract failed chunk {i}-{i+chunk_size}: {e}")

        # Dedup again
        deduped: dict[str, dict[str, Any]] = {}
        for ent in entities:
            k = _norm_name(ent["name"])
            if not k:
                continue
            if k not in deduped:
                deduped[k] = ent
            else:
                # merge evidence urls
                deduped[k]["evidence_urls"] = list(
                    {
                        *(deduped[k].get("evidence_urls") or []),
                        *(ent.get("evidence_urls") or []),
                    }
                )

        # Upsert
        now = datetime.utcnow()
        for ent in list(deduped.values())[:max_entities]:
            name = ent["name"].strip()
            if entity_type == "cooperative":
                stmt = select(Cooperative).where(
                    func.lower(Cooperative.name) == name.lower()
                )
                obj = db.scalar(stmt)
                if obj is None:
                    continue
                obj = cast(Any, obj)
                obj = cast(Any, obj)
                is_new = obj is None
                if is_new:
                    obj = Cooperative(
                        name=name, status="active", next_action="In Recherche"
                    )
                    db.add(obj)
                # Fill only if empty (conservative)
                if obj is None:
                    continue
                obj = cast(Any, obj)
                if ent.get("region") and not obj.region:
                    obj.region = str(ent["region"])[:255]
                if ent.get("website") and not obj.website:
                    obj.website = str(ent["website"])[:500]
                if ent.get("contact_email") and not obj.contact_email:
                    obj.contact_email = str(ent["contact_email"])[:320]
                if ent.get("notes"):
                    obj.notes = (obj.notes or "").strip()
                    add = str(ent["notes"]).strip()
                    if add and add not in (obj.notes or ""):
                        obj.notes = (
                            (obj.notes + "\n\n" + add).strip() if obj.notes else add
                        )
                obj.meta = obj.meta or {}
                obj.meta.setdefault("discovery", {})
                obj.meta["discovery"].update(
                    {"provider": "perplexity", "last_run": now.isoformat()}
                )
                if not dry_run:
                    db.commit()
                    db.refresh(obj)
                entity_id = obj.id if obj else None
            else:
                stmt = select(Roaster).where(func.lower(Roaster.name) == name.lower())  # type: ignore[assignment]
                obj = db.scalar(stmt)
                if obj is None:
                    continue
                obj = cast(Any, obj)
                obj = cast(Any, obj)
                is_new = obj is None
                if is_new:
                    obj = Roaster(  # type: ignore[assignment]
                        name=name, status="active", next_action="In Recherche"
                    )
                    db.add(obj)
                if obj is None:
                    continue
                obj = cast(Any, obj)
                if ent.get("region") and not obj.city:
                    obj.city = str(ent["region"])[:255]
                if ent.get("website") and not obj.website:
                    obj.website = str(ent["website"])[:500]
                if ent.get("contact_email") and not obj.contact_email:
                    obj.contact_email = str(ent["contact_email"])[:320]
                if ent.get("notes"):
                    obj.notes = (obj.notes or "").strip()
                    add = str(ent["notes"]).strip()
                    if add and add not in (obj.notes or ""):
                        obj.notes = (
                            (obj.notes + "\n\n" + add).strip() if obj.notes else add
                        )
                obj.meta = obj.meta or {}
                obj.meta.setdefault("discovery", {})
                obj.meta["discovery"].update(
                    {"provider": "perplexity", "last_run": now.isoformat()}
                )
                if not dry_run:
                    db.commit()
                    db.refresh(obj)
                entity_id = obj.id if obj else None

            # evidence URLs
            ev_urls = list(dict.fromkeys((ent.get("evidence_urls") or [])))
            if entity_id and ev_urls and not dry_run:
                for u in ev_urls[:10]:
                    try:
                        ev = EntityEvidence(
                            entity_type=entity_type,
                            entity_id=entity_id,
                            source_id=src.id,
                            evidence_url=u,
                            extracted_at=now,
                            meta={"provider": "perplexity"},
                        )
                        db.add(ev)
                        db.commit()
                    except Exception:
                        db.rollback()
                        # ignore duplicates

            if is_new:
                created += 1
            else:
                updated += 1

        # If dry-run, rollback everything
        if dry_run:
            db.rollback()

    except PerplexityError as e:
        errors.append(str(e))
    finally:
        client.close()

    return {
        "entity_type": entity_type,
        "country": country,
        "dry_run": dry_run,
        "created": created,
        "updated": updated,
        "skipped": skipped,
        "errors": errors,
    }






